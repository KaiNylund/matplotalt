{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu.metrics import BLEU, CHRF, TER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
    "bleu = BLEU()\n",
    "chrf = CHRF()\n",
    "\n",
    "def compute_scores(refs, sample):\n",
    "    ref_scores = []\n",
    "    for ref in refs:\n",
    "        cur_scores = {}\n",
    "        rouge_scores = rscorer.score(ref, sample)\n",
    "        for rouge_metric, score_result in rouge_scores.items():\n",
    "            cur_scores[f\"{rouge_metric}_p\"] = score_result.precision\n",
    "            cur_scores[f\"{rouge_metric}_r\"] = score_result.recall\n",
    "            cur_scores[f\"{rouge_metric}_f1\"] = score_result.fmeasure\n",
    "        cur_scores[\"bleu\"] = bleu.corpus_score([sample], [[ref]]).score\n",
    "        cur_scores[\"chrf\"] = chrf.corpus_score([sample], [[ref]]).score\n",
    "        ref_scores.append(cur_scores)\n",
    "    return ref_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'float'>,\n",
      "            {'bleu': 16.540560622991155,\n",
      "             'chrf': 44.772266293347364,\n",
      "             'rouge1_f1': 0.5725829051880719,\n",
      "             'rouge1_p': 0.601090520198768,\n",
      "             'rouge1_r': 0.5616816763796633,\n",
      "             'rouge2_f1': 0.35125909567922414,\n",
      "             'rouge2_p': 0.36745948390107247,\n",
      "             'rouge2_r': 0.3457935983653739,\n",
      "             'rougeL_f1': 0.41039645715226974,\n",
      "             'rougeL_p': 0.4292843430994817,\n",
      "             'rougeL_r': 0.40405176601888415,\n",
      "             'rougeLsum_f1': 0.4176004554190515,\n",
      "             'rougeLsum_p': 0.43819347649255214,\n",
      "             'rougeLsum_r': 0.409780888459035})\n",
      "defaultdict(<class 'float'>,\n",
      "            {'bleu': 10.350772219532587,\n",
      "             'chrf': 43.45331353767975,\n",
      "             'rouge1_f1': 0.37183044947375515,\n",
      "             'rouge1_p': 0.25689516175819455,\n",
      "             'rouge1_r': 0.7025395276006272,\n",
      "             'rouge2_f1': 0.19992057400852206,\n",
      "             'rouge2_p': 0.13727355646126066,\n",
      "             'rouge2_r': 0.3843128175910299,\n",
      "             'rougeL_f1': 0.2711147953521689,\n",
      "             'rougeL_p': 0.18662293124195123,\n",
      "             'rougeL_r': 0.51730520540274,\n",
      "             'rougeLsum_f1': 0.2971175601726808,\n",
      "             'rougeLsum_p': 0.204717151679193,\n",
      "             'rougeLsum_r': 0.5655559314664655})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vistext_captions = np.load(\"./vistext_to_mpl/vistext_id_to_captions.npy\", allow_pickle=True).item()\n",
    "matplotalt_captions = np.load(\"./vistext_to_mpl/vistext_id_to_matplotalt_captions.npy\", allow_pickle=True).item()\n",
    "gpt4v_captions = np.load(\"./vistext_to_mpl/vistext_id_to_gpt4v_captions.npy\", allow_pickle=True).item()\n",
    "\n",
    "heuristic_scores = {}\n",
    "gpt4v_scores = {}\n",
    "heuristic_avg_scores = defaultdict(float)\n",
    "gpt4v_avg_scores = defaultdict(float)\n",
    "num_samples = len(gpt4v_captions)\n",
    "for chart_id in tqdm(gpt4v_captions.keys()):\n",
    "    # Combine L1 and L2+ captions:\n",
    "    ref_captions = [cap[\"L1\"] + cap[\"L2L3\"] for cap in vistext_captions[chart_id]]\n",
    "    num_refs = len(ref_captions)\n",
    "    heuristic_scores[chart_id] = compute_scores(ref_captions, matplotalt_captions[chart_id])\n",
    "    gpt4v_scores[chart_id] = compute_scores(ref_captions, gpt4v_captions[chart_id])\n",
    "    for caption_score in heuristic_scores[chart_id]:\n",
    "        for metric, metric_score in caption_score.items():\n",
    "            heuristic_avg_scores[metric] += (metric_score / num_refs)\n",
    "    for caption_score in gpt4v_scores[chart_id]:\n",
    "        for metric, metric_score in caption_score.items():\n",
    "            gpt4v_avg_scores[metric] += (metric_score / num_refs)\n",
    "\n",
    "for metric in heuristic_avg_scores.keys():\n",
    "    heuristic_avg_scores[metric] /= num_samples\n",
    "for metric in gpt4v_avg_scores.keys():\n",
    "    gpt4v_avg_scores[metric] /= num_samples\n",
    "\n",
    "pprint(heuristic_avg_scores)\n",
    "pprint(gpt4v_avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'float'>,\n",
      "            {'bleu': 17.0387738558721,\n",
      "             'chrf': 45.875211312538575,\n",
      "             'rouge1_f1': 0.5800621156121178,\n",
      "             'rouge1_p': 0.6069744788336024,\n",
      "             'rouge1_r': 0.5776650606422175,\n",
      "             'rouge2_f1': 0.3582866181251763,\n",
      "             'rouge2_p': 0.36913674290073617,\n",
      "             'rouge2_r': 0.3585578079986235,\n",
      "             'rougeL_f1': 0.41689796531692375,\n",
      "             'rougeL_p': 0.4326594501660035,\n",
      "             'rougeL_r': 0.41657283308095644,\n",
      "             'rougeLsum_f1': 0.4244132600159041,\n",
      "             'rougeLsum_p': 0.44340128479715285,\n",
      "             'rougeLsum_r': 0.4219865011345621})\n",
      "defaultdict(<class 'float'>,\n",
      "            {'bleu': 10.574568916494481,\n",
      "             'chrf': 43.96768531636082,\n",
      "             'rouge1_f1': 0.3796436157827411,\n",
      "             'rouge1_p': 0.26492455452770897,\n",
      "             'rouge1_r': 0.7125593471154261,\n",
      "             'rouge2_f1': 0.20359026310199727,\n",
      "             'rouge2_p': 0.14028984477307196,\n",
      "             'rouge2_r': 0.3952895395028076,\n",
      "             'rougeL_f1': 0.2749121060781977,\n",
      "             'rougeL_p': 0.19079529043026416,\n",
      "             'rougeL_r': 0.5274598445204773,\n",
      "             'rougeLsum_f1': 0.30205761812418347,\n",
      "             'rougeLsum_p': 0.2098736365158913,\n",
      "             'rougeLsum_r': 0.5769089315545864})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "heuristic_scores = {}\n",
    "gpt4v_scores = {}\n",
    "heuristic_avg_max_scores = defaultdict(float)\n",
    "gpt4v_avg_max_scores = defaultdict(float)\n",
    "num_samples = len(gpt4v_captions)\n",
    "for chart_id in tqdm(gpt4v_captions.keys()):\n",
    "    # Combine L1 and L2+ captions:\n",
    "    ref_captions = [cap[\"L1\"] + cap[\"L2L3\"] for cap in vistext_captions[chart_id]]\n",
    "    num_imgs = len(set())\n",
    "    heuristic_scores[chart_id] = compute_scores(ref_captions, matplotalt_captions[chart_id])\n",
    "    gpt4v_scores[chart_id] = compute_scores(ref_captions, gpt4v_captions[chart_id])\n",
    "\n",
    "    max_heuristic_scores = heuristic_scores[chart_id][0]\n",
    "    for caption_score in heuristic_scores[chart_id]:\n",
    "        for metric, metric_score in caption_score.items():\n",
    "            max_heuristic_scores[metric] = max(max_heuristic_scores[metric], metric_score)\n",
    "    for metric, metric_score in caption_score.items():\n",
    "        heuristic_avg_max_scores[metric] += max_heuristic_scores[metric]\n",
    "\n",
    "    max_gpt4v_scores = gpt4v_scores[chart_id][0]\n",
    "    for caption_score in gpt4v_scores[chart_id]:\n",
    "        for metric, metric_score in caption_score.items():\n",
    "            max_gpt4v_scores[metric] = max(max_gpt4v_scores[metric], metric_score)\n",
    "    for metric, metric_score in caption_score.items():\n",
    "        gpt4v_avg_max_scores[metric] += max_gpt4v_scores[metric]\n",
    "\n",
    "for metric in heuristic_avg_max_scores.keys():\n",
    "    heuristic_avg_max_scores[metric] /= num_samples\n",
    "for metric in gpt4v_avg_max_scores.keys():\n",
    "    gpt4v_avg_max_scores[metric] /= num_samples\n",
    "\n",
    "pprint(heuristic_avg_max_scores)\n",
    "pprint(gpt4v_avg_max_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 56.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'float'>, {'rouge1_p': 0.19284805854368084, 'rouge1_r': 0.4738301760868292, 'rouge1_f1': 0.2596389568612051, 'rougeL_p': 0.12798409638061822, 'rougeL_r': 0.3230583730233787, 'rougeL_f1': 0.17353513826269723, 'bleu': 2.695336052982247, 'chrf': 31.3504902398399})\n",
      "defaultdict(<class 'float'>, {'rouge1_p': 0.08714004571220785, 'rouge1_r': 0.6055190621662573, 'rouge1_f1': 0.1476649683244705, 'rougeL_p': 0.06330650358703468, 'rougeL_r': 0.46128790448466056, 'rougeL_f1': 0.10805419457886326, 'bleu': 1.7124975383414258, 'chrf': 23.32196328069717})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "heuristic_scores = {}\n",
    "gpt4v_scores = {}\n",
    "heuristic_avg_scores = defaultdict(float)\n",
    "gpt4v_avg_scores = defaultdict(float)\n",
    "num_samples = len(gpt4v_captions)\n",
    "for chart_id in tqdm(gpt4v_captions.keys()):\n",
    "    # Just L2+ captions:\n",
    "    ref_captions = [cap[\"L2L3\"] for cap in vistext_captions[chart_id]]\n",
    "    num_refs = len(ref_captions)\n",
    "    heuristic_scores[chart_id] = compute_scores(ref_captions, matplotalt_captions[chart_id])\n",
    "    gpt4v_scores[chart_id] = compute_scores(ref_captions, gpt4v_captions[chart_id])\n",
    "    for caption_score in heuristic_scores[chart_id]:\n",
    "        for metric, metric_score in caption_score.items():\n",
    "            heuristic_avg_scores[metric] += (metric_score / num_refs)\n",
    "    for caption_score in gpt4v_scores[chart_id]:\n",
    "        for metric, metric_score in caption_score.items():\n",
    "            gpt4v_avg_scores[metric] += (metric_score / num_refs)\n",
    "\n",
    "for metric in heuristic_avg_scores.keys():\n",
    "    heuristic_avg_scores[metric] /= num_samples\n",
    "for metric in gpt4v_avg_scores.keys():\n",
    "    gpt4v_avg_scores[metric] /= num_samples\n",
    "\n",
    "print(heuristic_avg_scores)\n",
    "print(gpt4v_avg_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
