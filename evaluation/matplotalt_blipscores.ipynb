{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please follow the instructions to set up the BLIP repo (hosted at [their github](https://github.com/salesforce/BLIP) before running this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from models.blip_itm import blip_itm\n",
    "\n",
    "image_size = 384\n",
    "\n",
    "# Load BLIP model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth'\n",
    "model = blip_itm(pretrained=model_url, image_size=image_size, vit='base')\n",
    "model.eval()\n",
    "model = model.to(device='cpu')\n",
    "\n",
    "def load_demo_image(image_path, device):\n",
    "    raw_image = Image.open(image_path).convert('RGB')\n",
    "    w,h = raw_image.size\n",
    "    display(raw_image.resize((w//5,h//5)))\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "    image = transform(raw_image).unsqueeze(0).to(device)\n",
    "    return image\n",
    "\n",
    "\n",
    "def blip_score(image, caption):\n",
    "    itm_output = model(image,caption,match_head='itm')\n",
    "    itm_score = torch.nn.functional.softmax(itm_output,dim=1)[:,1] # Text matching prob\n",
    "    itc_score = model(image,caption,match_head='itc') # Cos sim between img and caption\n",
    "    return itm_score, itc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_id', 'human', 'heuristic', 'gpt-4-turbo-L4',\n",
      "       'gpt-4-turbo-alt-L4', 'gpt-4-turbo-table-L4', 'gpt-4-turbo-L3',\n",
      "       'gpt-4-turbo-alt-L3', 'gpt-4-turbo-table-L3',\n",
      "       'gpt-4-turbo-table-alt-L3'],\n",
      "      dtype='object')\n",
      "Index(['heuristic', 'gpt-4-turbo-L3-225', 'gpt-4-turbo-alt-L3-225',\n",
      "       'figure_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "vistext_imgs_path = \"../matplotalt/evaluation/vistext_eval/matplotlib_ver_imgs/\"\n",
    "gallery_imgs_path = \"../matplotalt/evaluation/matplotlib_gallery/alt_figs/\"\n",
    "\n",
    "# Load vistext and matplotlib gallery captions\n",
    "vistext_captions_df = pd.read_json(\"../matplotalt/evaluation/vistext_eval/vistext_id_to_combined_captions.jsonl\", orient='records', lines=True)\n",
    "gallery_captions_df = pd.read_json(\"../matplotalt/evaluation/matplotlib_gallery/mpl_gallery_combined_captions_shuffled.jsonl\", orient='records', lines=True)\n",
    "print(vistext_captions_df.columns)\n",
    "print(gallery_captions_df.columns)\n",
    "\n",
    "vistext_col_names = ['human', 'heuristic', 'gpt-4-turbo-L4',\n",
    "                     'gpt-4-turbo-alt-L4', 'gpt-4-turbo-table-L4',\n",
    "                     'gpt-4-turbo-L3', 'gpt-4-turbo-alt-L3',\n",
    "                     'gpt-4-turbo-table-L3', 'gpt-4-turbo-table-alt-L3']\n",
    "\n",
    "gallery_col_names = ['heuristic', 'gpt-4-turbo-L3-225', 'gpt-4-turbo-alt-L3-225']\n",
    "gallery_id_to_probs = defaultdict(list)\n",
    "gallery_id_to_cos_sims = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate vistext blipscores\n",
    "vistext_id_to_probs = defaultdict(list)\n",
    "vistext_id_to_cos_sims = defaultdict(list)\n",
    "for _, row in tqdm(vistext_captions_df.iterrows()):\n",
    "    fig_id = row[\"image_id\"]\n",
    "    image = load_demo_image(image_path=f\"{vistext_imgs_path}{fig_id}.png\", device=device);\n",
    "    for desc_type in vistext_col_names:\n",
    "        if desc_type in row and row[desc_type] is not None:\n",
    "            captions = row[desc_type]\n",
    "            desc_type_probs = []\n",
    "            desc_type_sims = []\n",
    "            for cap in captions:\n",
    "                cap = cap.replace(\"This description was generated by a language model.\", \"\")\n",
    "                cap_prob, cap_sim = blip_score(image, cap)\n",
    "                desc_type_probs.append(cap_prob.item())\n",
    "                desc_type_sims.append(cap_sim.item())\n",
    "                #print(cap_prob.item(), cap_sim.item())\n",
    "            if len(desc_type_probs) == 1:\n",
    "                vistext_id_to_probs[fig_id].append(desc_type_probs[0])\n",
    "                vistext_id_to_cos_sims[fig_id].append(desc_type_sims[0])\n",
    "            else:\n",
    "                vistext_id_to_probs[fig_id].append(desc_type_probs)\n",
    "                vistext_id_to_cos_sims[fig_id].append(desc_type_sims)\n",
    "        else:\n",
    "            vistext_id_to_probs[fig_id].append(np.nan)\n",
    "            vistext_id_to_cos_sims[fig_id].append(np.nan)\n",
    "    vistext_probs_df = pd.DataFrame.from_dict(vistext_id_to_probs, orient='index',\n",
    "                                              columns=[cn + \"-prob\" for cn in vistext_col_names])\n",
    "    vistext_sims_df = pd.DataFrame.from_dict(vistext_id_to_cos_sims, orient='index',\n",
    "                                              columns=[cn + \"-cos-sim\" for cn in vistext_col_names])\n",
    "    vistext_probs_df['figure_id'] = vistext_probs_df.index\n",
    "    vistext_sims_df['figure_id'] = vistext_sims_df.index\n",
    "    vistext_blipscore_df = pd.merge(vistext_probs_df, vistext_sims_df, on=\"figure_id\", how=\"outer\")\n",
    "    vistext_blipscore_df.to_json(\"./vistext_blipscores.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate matplotlib gallery blipscores\n",
    "gallery_id_to_probs = defaultdict(list)\n",
    "gallery_id_to_cos_sims = defaultdict(list)\n",
    "for _, row in tqdm(gallery_captions_df.iterrows()):\n",
    "    fig_id = row[\"figure_id\"]\n",
    "    image = load_demo_image(image_path=f\"{gallery_imgs_path}nb_{fig_id}.jpg\", device=device);\n",
    "    for desc_type in gallery_col_names:\n",
    "        if desc_type in row and row[desc_type] is not None:\n",
    "            caption = row[desc_type]\n",
    "            caption = caption.replace(\"This description was generated by a language model.\", \"\")\n",
    "            cap_prob, cap_sim = blip_score(image, caption)\n",
    "            gallery_id_to_probs[fig_id].append(cap_prob.item())\n",
    "            gallery_id_to_cos_sims[fig_id].append(cap_sim.item())\n",
    "        else:\n",
    "            gallery_id_to_probs[fig_id].append(np.nan)\n",
    "            gallery_id_to_cos_sims[fig_id].append(np.nan)\n",
    "    gallery_probs_df = pd.DataFrame.from_dict(gallery_id_to_probs, orient='index',\n",
    "                                              columns=[cn + \"-prob\" for cn in gallery_col_names])\n",
    "    gallery_sims_df = pd.DataFrame.from_dict(gallery_id_to_cos_sims, orient='index',\n",
    "                                              columns=[cn + \"-cos-sim\" for cn in gallery_col_names])\n",
    "    gallery_probs_df['figure_id'] = gallery_probs_df.index\n",
    "    gallery_sims_df['figure_id'] = gallery_sims_df.index\n",
    "    gallery_blipscore_df = pd.merge(gallery_probs_df, gallery_sims_df, on=\"figure_id\", how=\"outer\")\n",
    "    gallery_blipscore_df.to_json(\"./gallery_blipscores.jsonl\", orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib gallery average blipscores: ---------------------------------------------------\n",
      "heuristic-prob: 0.9269914858376802\n",
      "gpt-4-turbo-L3-225-prob: 0.9765950807588017\n",
      "gpt-4-turbo-alt-L3-225-prob: 0.9629381057338335\n",
      "heuristic-cos-sim: 0.4262510082911496\n",
      "gpt-4-turbo-L3-225-cos-sim: 0.48792191035118865\n",
      "gpt-4-turbo-alt-L3-225-cos-sim: 0.4717426163640188\n",
      "\n",
      "Vistext gallery average blipscores: ---------------------------------------------------\n",
      "human-prob: 0.9999340540832944\n",
      "heuristic-prob: 0.9999378859456164\n",
      "gpt-4-turbo-L4-prob: 0.9979466208948184\n",
      "gpt-4-turbo-alt-L4-prob: 0.9989826586011643\n",
      "gpt-4-turbo-table-L4-prob: 0.999617625027895\n",
      "gpt-4-turbo-L3-prob: 0.9984933224901591\n",
      "gpt-4-turbo-alt-L3-prob: 0.9999297380988633\n",
      "gpt-4-turbo-table-L3-prob: 0.9977022701246446\n",
      "gpt-4-turbo-table-alt-L3-prob: 0.9999251992644735\n",
      "human-cos-sim: 0.4992284501310739\n",
      "heuristic-cos-sim: 0.5037227192906295\n",
      "gpt-4-turbo-L4-cos-sim: 0.5023882733156307\n",
      "gpt-4-turbo-alt-L4-cos-sim: 0.5049227834571891\n",
      "gpt-4-turbo-table-L4-cos-sim: 0.5019848114224496\n",
      "gpt-4-turbo-L3-cos-sim: 0.5038625215794764\n",
      "gpt-4-turbo-alt-L3-cos-sim: 0.5079316001271281\n",
      "gpt-4-turbo-table-L3-cos-sim: 0.5049805727435295\n",
      "gpt-4-turbo-table-alt-L3-cos-sim: 0.5081209050536568\n"
     ]
    }
   ],
   "source": [
    "print(\"Matplotlib gallery average blipscores: ---------------------------------------------------\")\n",
    "for col_name in gallery_blipscore_df.columns:\n",
    "    if col_name != \"figure_id\":\n",
    "        col_values = list(gallery_blipscore_df[col_name])\n",
    "        col_values = [np.nanmean([v]) for v in col_values]\n",
    "        mean_val = np.nanmean(col_values)\n",
    "        print(f\"{col_name}: {mean_val}\")\n",
    "print()\n",
    "print(\"Vistext gallery average blipscores: ---------------------------------------------------\")\n",
    "for col_name in vistext_blipscore_df.columns:\n",
    "    if col_name != \"figure_id\":\n",
    "        col_values = list(vistext_blipscore_df[col_name])\n",
    "        col_values = [np.nanmean([v]) for v in col_values]\n",
    "        mean_val = np.nanmean(col_values)\n",
    "        print(f\"{col_name}: {mean_val}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
