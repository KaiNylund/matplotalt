{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch.nn as  nn\n",
    "import torchvision.transforms as standard_transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_chart_type = {\n",
    "    -1: \"other\",\n",
    "    1: \"line\",\n",
    "    2: \"scatter\",\n",
    "    4: \"bar\",\n",
    "    7: \"heat_map\",\n",
    "    9: \"box-plot\",\n",
    "    10: \"bubble\",\n",
    "    13: \"sankey\",\n",
    "    14: \"chord\",\n",
    "    15: \"radial\",\n",
    "    16: \"area\",\n",
    "    18: \"donut\",\n",
    "    19: \"choropleth\",\n",
    "    22: \"treemap\",\n",
    "    29: \"pie\",\n",
    "    31: \"stream_graph\",\n",
    "    33: \"hexabin\",\n",
    "    35: \"graph\",\n",
    "    37: \"parallel_coordinates\",\n",
    "    38: \"sunburst\",\n",
    "    39: \"waffle\",\n",
    "    40: \"voronoi\",\n",
    "    41: \"word_cloud\",\n",
    "    60: \"contour\",\n",
    "    61: \"filled-line\",\n",
    "    62: \"scattergeo\"\n",
    "}\n",
    "\n",
    "old_id_to_new = {\n",
    "    -1: 1,\n",
    "    1: 1,\n",
    "    2: 2,\n",
    "    4: 3,\n",
    "    7: 4,\n",
    "    9: 5,\n",
    "    10: 6,\n",
    "    13: 7,\n",
    "    14: 8,\n",
    "    15: 9,\n",
    "    16: 10,\n",
    "    18: 11,\n",
    "    19: 12,\n",
    "    22: 13,\n",
    "    29: 14,\n",
    "    31: 15,\n",
    "    33: 16,\n",
    "    35: 17,\n",
    "    37: 18,\n",
    "    38: 19,\n",
    "    39: 20,\n",
    "    40: 21,\n",
    "    41: 22,\n",
    "    60: 23,\n",
    "    61: 24,\n",
    "    62: 25\n",
    "}\n",
    "\n",
    "\n",
    "chart_type_to_id = {v: k for k, v in id_to_chart_type.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/76/70fs0djn7lb2y98w5_hxbtx00000gn/T/ipykernel_83641/727284079.py:3: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  standard_transforms.Resize((128, 128), interpolation=Image.ANTIALIAS),\n"
     ]
    }
   ],
   "source": [
    "mean_std = ( [.485, .456, .406], [.229, .224, .225])\n",
    "fig_class_trasform = standard_transforms.Compose([\n",
    "    standard_transforms.Resize((128, 128), interpolation=Image.ANTIALIAS),\n",
    "    standard_transforms.ToTensor(),\n",
    "    standard_transforms.Normalize(*mean_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15232/15232 [01:16<00:00, 197.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of docs with parsing errors: 0.0008534663865546219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22557/22557 [01:41<00:00, 222.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of docs with parsing errors: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1440/1440 [00:12<00:00, 113.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of docs with parsing errors: 0.14305555555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2733/2733 [00:07<00:00, 342.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of docs with parsing errors: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 697/697 [00:02<00:00, 345.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of docs with parsing errors: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dirs = [\"./svg_datasets/plotly_export\", \"./svg_datasets/chartblocks\", \"./svg_datasets/d3_clean\", \"./svg_datasets/graphiq_clean\", \"./svg_datasets/fusion_clean\"]\n",
    "#[\"./svg_datasets/plotly_export\", \"./svg_datasets/chartblocks\", \"./svg_datasets/d3_clean\", \"./svg_datasets/graphiq_clean\"]\n",
    "X = []\n",
    "y = []\n",
    "for charts_dir in data_dirs:\n",
    "    labels_path = f\"{charts_dir}/urls.txt\"\n",
    "    id_to_label = {}\n",
    "\n",
    "    with open(labels_path) as labels_file:\n",
    "        for label_line in labels_file.readlines():\n",
    "            label_split = label_line.split(\" \")\n",
    "            sample_id = label_split[0]\n",
    "            sample_label = label_split[2].split(\",\")[0]\n",
    "            if \"plotly\" in charts_dir:\n",
    "                id_to_label[sample_id] = chart_type_to_id[sample_label.replace(\"\\n\", \"\")]\n",
    "            else:\n",
    "                id_to_label[sample_id] = int(sample_label.replace(\"\\n\", \"\"))\n",
    "\n",
    "    err_count = 0\n",
    "    processed_count = 0\n",
    "    for chart_id, chart_label in tqdm(id_to_label.items()):\n",
    "        img_path = f\"{charts_dir}/images/{chart_id}.png\"\n",
    "        #print(svg_path)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            #svg_feature_arr = fig_class_trasform(image).numpy().flatten()\n",
    "            X.append(image)\n",
    "            y.append(old_id_to_new[chart_label])\n",
    "            #processed_count += 1\n",
    "            #if processed_count > 1000:\n",
    "            #    break\n",
    "        except Exception as err:\n",
    "            err_count += 1\n",
    "\n",
    "    print(f\"Percentage of docs with parsing errors: {err_count / len(id_to_label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4244 8488 29707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/76/70fs0djn7lb2y98w5_hxbtx00000gn/T/ipykernel_83641/1133528323.py:15: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  X = np.array(X)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dev_pct = 0.1\n",
    "test_pct = 0.2\n",
    "shuffled_idxs = np.random.choice(len(y), size=len(y), replace=False)\n",
    "\n",
    "num_dev_elements = int(len(y) * dev_pct)\n",
    "num_test_elements = int(len(y) * test_pct)\n",
    "num_train_elements = int(len(y) * (1.0 - dev_pct - test_pct))\n",
    "\n",
    "print(num_dev_elements, num_test_elements, num_train_elements)\n",
    "\n",
    "train_idxs = shuffled_idxs[:num_train_elements]\n",
    "dev_idxs = shuffled_idxs[num_train_elements:(num_train_elements + num_dev_elements)]\n",
    "test_idxs = shuffled_idxs[(num_train_elements + num_dev_elements + 1):]\n",
    "\n",
    "X = np.array(X)\n",
    "X[X > 1e300] = 0.0\n",
    "X[X < -1e300] = 0.0\n",
    "X[np.isnan(X)] = 0.0\n",
    "X_train = X[train_idxs]\n",
    "X_dev = X[dev_idxs]\n",
    "X_test = X[test_idxs]\n",
    "\n",
    "y = np.array(y)\n",
    "y_train = y[train_idxs]\n",
    "y_dev = y[dev_idxs]\n",
    "y_test = y[test_idxs]\n",
    "\n",
    "train_ds = Dataset.from_dict({\"image\": list(X_train), \"label\": list(y_train)})\n",
    "dev_ds = Dataset.from_dict({\"image\": list(X_dev), \"label\": list(y_dev)})\n",
    "test_ds = Dataset.from_dict({\"image\": list(X_test), \"label\": list(y_test)})\n",
    "\n",
    "full_dataset = DatasetDict({\"train\": train_ds, \"dev\": dev_ds, \"test\": test_ds})\n",
    "full_dataset.save_to_disk(\"./beagle_chart_to_label.hf\")\n",
    "\n",
    "#np.save(\"./svg_datasets/imgs_X_train\", X_train)\n",
    "#np.save(\"./svg_datasets/imgs_X_dev\", X_dev)\n",
    "#np.save(\"./svg_datasets/imgs_X_test\", X_test)\n",
    "#np.save(\"./svg_datasets/imgs_y_train\", y_train)\n",
    "#np.save(\"./svg_datasets/imgs_y_dev\", y_dev)\n",
    "#np.save(\"./svg_datasets/imgs_y_test\", y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
